"""Agentic MCP server code generator for Blaxel — purely LLM-driven.

The entire server.py is generated by DeepSeek-V3 via the Featherless API
in a single pass. No scaffolding, no stitching — pure generation.
Validated with ast.parse() and auto-repaired if needed.
Generated servers use mcp.server.fastmcp.FastMCP for Blaxel hosting.

Provider: Featherless (FEATHERLESS_API_KEY → deepseek-ai/DeepSeek-V3-0324)
"""

from __future__ import annotations

import ast
import json
import os
import re
import textwrap
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import httpx

from .logger import get_logger, log_stage
from .models import (
    APISpec,
    AuthScheme,
    HttpMethod,
    ParamLocation,
    SafetyLevel,
    ToolDefinition,
    ToolParam,
)


# ═══════════════════════════════════════════════════════════════════════════
# LLM provider — Featherless (DeepSeek-V3-0324)
# ═══════════════════════════════════════════════════════════════════════════

FEATHERLESS_BASE_URL = "https://api.featherless.ai/v1"
FEATHERLESS_MODEL = "deepseek-ai/DeepSeek-V3-0324"


def _call_llm(prompt: str, *, system_instruction: str | None = None,
              temperature: float = 0.15, max_tokens: int = 16384) -> str:
    """Call DeepSeek-V3 via Featherless API."""
    logger = get_logger()
    api_key = os.getenv("FEATHERLESS_API_KEY", "")
    if not api_key:
        raise RuntimeError(
            "FEATHERLESS_API_KEY is not set. "
            "Add it to your .env file: FEATHERLESS_API_KEY=your-key-here"
        )

    model = os.getenv("FEATHERLESS_MODEL", FEATHERLESS_MODEL)
    logger.info("  LLM → Featherless (%s)", model)

    messages: list[dict[str, str]] = []
    if system_instruction:
        messages.append({"role": "system", "content": system_instruction})
    messages.append({"role": "user", "content": prompt})

    with httpx.Client(timeout=180.0) as c:
        resp = c.post(
            f"{FEATHERLESS_BASE_URL}/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json={
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
            },
        )
        resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


def _extract_code(text: str) -> str:
    """Pull the first ```python ... ``` block (or the whole text)."""
    m = re.search(r"```python\s*\n(.*?)```", text, re.DOTALL)
    if m:
        return m.group(1).strip()
    m = re.search(r"```\w*\s*\n(.*?)```", text, re.DOTALL)
    if m:
        return m.group(1).strip()
    return text.strip()


# ═══════════════════════════════════════════════════════════════════════════
# System prompt — tells the LLM exactly how to generate a complete server
# ═══════════════════════════════════════════════════════════════════════════

_SYSTEM_PROMPT = textwrap.dedent("""\
    You are an expert Python developer. You generate complete, production-ready
    MCP server files using the `mcp.server.fastmcp.FastMCP` framework for Blaxel hosting.

    ## Rules

    1. Output ONLY Python code inside a single ```python fence. No prose.
    2. The file must be a complete, runnable Python module.
    3. Use these imports:
       ```
       from __future__ import annotations
       import json, os
       from typing import Any
       import httpx
       from mcp.server.fastmcp import FastMCP
       ```
    4. Configuration via environment variables:
       - `<PREFIX>_BASE_URL` — base URL of the upstream API
       - `<PREFIX>_API_KEY`  — API key (may be empty)
    5. Define a `_headers()` helper that returns auth headers.
       - For Bearer auth: `{"Authorization": f"Bearer {API_KEY}"}`
       - For apiKey auth: use the correct header name with the raw key value
       - Always include `Content-Type` and `Accept` as `application/json`
    6. Define an `async def _request(method, path, *, params=None, body=None) -> str`
       helper that:
       - Builds the full URL from BASE_URL + path
       - Uses `httpx.AsyncClient` with `timeout=30.0`
       - Returns `json.dumps(resp.json(), indent=2)` on success
       - Returns `json.dumps({"error": ..., "status": ...})` on HTTP errors
       - Returns `json.dumps({"error": ...})` on other exceptions
    7. Create the FastMCP instance at module level:
       ```
       mcp = FastMCP("<name>", stateless_http=True,
                     host=os.getenv("HOST", "0.0.0.0"),
                     port=int(os.getenv("PORT", "80")))
       ```
    8. Each tool uses `@mcp.tool()` decorator.
       - Returns `str`.
       - Calls `await _request(...)`.
       - Required params first, optional params have `| None = None`.
       - Path params via f-strings: `f"/pets/{pet_id}"`.
       - Query params: build a `params` dict, skip None values.
       - Body params (POST/PUT/PATCH): build a `body` dict, skip None values.
       - DELETE: just call `_request("DELETE", path)`.
       - One-line docstring that includes the description and safety badge.
       - Safety badges in docstring: write → "[WRITES DATA]", destructive → "[DESTRUCTIVE]".
    9. At the bottom:
       ```
       if __name__ == "__main__":
           mcp.run(transport="streamable-http")
       ```
    10. Type mapping: string→str, integer→int, number→float, boolean→bool,
        array→list, object→dict

    ## Example complete server

    ```python
    from __future__ import annotations
    import json, os
    from typing import Any
    import httpx
    from mcp.server.fastmcp import FastMCP

    BASE_URL = os.getenv("PETSTORE_API_BASE_URL", "https://petstore.example.com/api/v1")
    API_KEY  = os.getenv("PETSTORE_API_API_KEY", "")

    mcp = FastMCP("petstore", stateless_http=True,
                  host=os.getenv("HOST", "0.0.0.0"),
                  port=int(os.getenv("PORT", "80")))

    def _headers() -> dict[str, str]:
        h = {"Content-Type": "application/json", "Accept": "application/json"}
        if API_KEY:
            h["X-API-Key"] = API_KEY
        return h

    async def _request(method: str, path: str, *, params: dict[str, Any] | None = None,
                       body: dict[str, Any] | None = None) -> str:
        url = f"{BASE_URL}{path}"
        async with httpx.AsyncClient(timeout=30.0) as client:
            try:
                resp = await client.request(method, url, headers=_headers(),
                                            params=params, json=body if body else None)
                resp.raise_for_status()
                try:
                    return json.dumps(resp.json(), indent=2)
                except Exception:
                    return resp.text
            except httpx.HTTPStatusError as e:
                return json.dumps({"error": str(e), "status": e.response.status_code})
            except Exception as e:
                return json.dumps({"error": str(e)})

    @mcp.tool()
    async def search_pets(species: str | None = None, limit: int = 20) -> str:
        \"\"\"Search or list pets.\"\"\"        params: dict[str, Any] = {}
        if species is not None:
            params["species"] = species
        params["limit"] = limit
        return await _request("GET", "/pets", params=params)

    @mcp.tool()
    async def create_pet(name: str, species: str) -> str:
        \"\"\"Add a new pet. [WRITES DATA]\"\"\"        return await _request("POST", "/pets", body={"name": name, "species": species})

    @mcp.tool()
    async def delete_pet(pet_id: int) -> str:
        \"\"\"Permanently remove a pet. [DESTRUCTIVE]\"\"\"        return await _request("DELETE", f"/pets/{pet_id}")

    if __name__ == "__main__":
        mcp.run(transport="streamable-http")
    ```
""")


# ═══════════════════════════════════════════════════════════════════════════
# Prompt builders
# ═══════════════════════════════════════════════════════════════════════════


def _describe_tool(t: ToolDefinition) -> str:
    """Compact text description of one tool for the LLM prompt."""
    lines = [f"### Tool: `{t.name}`"]
    lines.append(f"- Description: {t.description}")
    lines.append(f"- Safety: {t.safety.value}")
    for ep in t.endpoints:
        lines.append(f"- Endpoint: {ep.method.value} {ep.path}")
    if t.params:
        lines.append("- Parameters:")
        for p in t.params:
            loc = "body"
            for ep in t.endpoints:
                for ep_p in ep.parameters:
                    if ep_p.name == p.name:
                        loc = ep_p.location.value
                        break
            lines.append(
                f"  - `{p.name}`: {p.json_type}, "
                f"{'required' if p.required else 'optional'}, "
                f"location={loc}"
                + (f", description: {p.description}" if p.description else "")
            )
    return "\n".join(lines)


def _describe_auth(schemes: list[AuthScheme]) -> str:
    """Describe auth schemes for the prompt."""
    if not schemes:
        return "No authentication required (but still support optional API_KEY with Bearer token)."
    parts = []
    for s in schemes:
        if s.scheme_type == "apiKey":
            hdr = s.header_name or s.name or "X-API-Key"
            parts.append(f"apiKey via header `{hdr}` (use raw API_KEY value, no Bearer prefix)")
        elif s.scheme_type in ("http", "oauth2"):
            parts.append(f"Bearer token via `Authorization: Bearer {{API_KEY}}`")
        else:
            parts.append(f"{s.scheme_type}: {s.name}")
    return "; ".join(parts)


def _build_server_prompt(
    spec: APISpec,
    tools: list[ToolDefinition],
    server_name: str,
    env_prefix: str,
) -> str:
    """Build the full prompt for generating server.py."""
    tool_specs = "\n\n".join(_describe_tool(t) for t in tools)
    auth_desc = _describe_auth(spec.auth_schemes)

    return textwrap.dedent(f"""\
        Generate a complete MCP server Python file for the following API.

        ## API Info
        - Title: {spec.title}
        - Version: {spec.version}
        - Base URL: {spec.base_url}
        - Description: {spec.description or "N/A"}
        - Auth: {auth_desc}

        ## Environment variable prefix: `{env_prefix}`
        - `{env_prefix}_BASE_URL` (default: `{spec.base_url}`)
        - `{env_prefix}_API_KEY` (default: `""`)

        ## Server name: `{server_name}`

        ## Tools ({len(tools)} total)

        {tool_specs}

        Generate the COMPLETE server.py file with ALL {len(tools)} tools.
        Return ONLY the Python code inside a ```python fence.
    """)


def _build_test_prompt(spec: APISpec, tools: list[ToolDefinition]) -> str:
    """Build the prompt for generating test_server.py."""
    tool_names = [t.name for t in tools]
    tool_test_list = "\n".join(
        f"   - test_{t.name}() — call tool \"{t.name}\" with sample args and verify it returns a string"
        for t in tools
    )
    return textwrap.dedent(f"""\
        Generate a COMPLETE test file for an MCP server with ALL {len(tools)} tools.

        API: {spec.title} v{spec.version}
        Tools ({len(tools)}): {tool_names}
        Server URL: http://127.0.0.1:8000/mcp

        Use `httpx` to test the MCP server HTTP endpoint:
        ```python
        import httpx
        async with httpx.AsyncClient() as client:
            resp = await client.post("http://127.0.0.1:80/mcp",
                json={{"jsonrpc": "2.0", "method": "tools/list", "id": 1}})
            tools = resp.json()
        ```

        You MUST include ALL of these test functions:
        1. test_list_tools() — verify exactly {len(tools)} tools registered
        2. test_tool_schemas() — each tool has name + description
{tool_test_list}
        3. main() that runs ALL tests
        4. if __name__ == "__main__" block

        IMPORTANT: Generate a test function for EVERY tool listed above.
        Do NOT skip any. Total expected: {2 + len(tools)} test functions.

        Return ONLY ```python code.
    """)


# ═══════════════════════════════════════════════════════════════════════════
# Validation & repair
# ═══════════════════════════════════════════════════════════════════════════


def _validate_python(code: str) -> tuple[bool, str]:
    """Return (is_valid, error_message)."""
    try:
        ast.parse(code)
        return True, ""
    except SyntaxError as e:
        return False, f"Line {e.lineno}: {e.msg}"


def _count_tools_in_code(code: str) -> int:
    """Count @mcp.tool() decorated functions in the code."""
    return len(re.findall(r"@mcp\.tool\(", code))


def _fix_code_with_llm(code: str, error: str, expected_tool_count: int) -> str:
    """Ask the LLM to fix broken generated code."""
    prompt = textwrap.dedent(f"""\
        The following Python code has errors. Fix them and return the corrected
        COMPLETE file inside a ```python fence.

        Error: {error}
        Expected number of @tool functions: {expected_tool_count}

        ```python
        {code}
        ```
    """)
    raw = _call_llm(prompt, system_instruction=_SYSTEM_PROMPT, max_tokens=16384)
    return _extract_code(raw)


# ═══════════════════════════════════════════════════════════════════════════
# Output model
# ═══════════════════════════════════════════════════════════════════════════


@dataclass
class GeneratedOutput:
    """Result of code generation."""
    server_code: str
    test_code: str
    requirements: str
    env_template: str
    server_name: str
    tool_count: int
    output_dir: Path | None = None


# ═══════════════════════════════════════════════════════════════════════════
# Main generator — purely LLM-driven
# ═══════════════════════════════════════════════════════════════════════════


def generate(
    spec: APISpec,
    tools: list[ToolDefinition],
    server_name: str | None = None,
    output_dir: str | Path | None = None,
) -> GeneratedOutput:
    """Generate a complete MCP server — entirely by the LLM.

    No scaffolding, no stitching. The LLM generates the full server.py
    in one shot, then we validate with ast.parse() and auto-repair if needed.
    """
    logger = get_logger()

    with log_stage("Agentic Code Generation"):
        name = server_name or re.sub(r"[^a-z0-9]+", "-", spec.title.lower()).strip("-")
        env_prefix = re.sub(r"[^A-Z0-9]+", "_", spec.title.upper()).strip("_")

        logger.info("Server: %s | Prefix: %s | Tools: %d", name, env_prefix, len(tools))
        logger.info("Provider: Featherless (DeepSeek-V3-0324)")

        # ── Step 1: Generate server.py (with retry loop) ────────────────
        logger.info("Step 1: Generating complete server.py via LLM...")
        prompt = _build_server_prompt(spec, tools, name, env_prefix)
        expected = len(tools)
        max_attempts = 3
        server_code = ""

        for attempt in range(1, max_attempts + 1):
            if attempt == 1:
                raw = _call_llm(prompt, system_instruction=_SYSTEM_PROMPT, max_tokens=16384)
                server_code = _extract_code(raw)
            else:
                logger.info("  Retry %d/%d — asking LLM to fix...", attempt, max_attempts)
                server_code = _fix_code_with_llm(server_code, issue_str, expected)

            valid, err = _validate_python(server_code)
            tool_count_in_code = _count_tools_in_code(server_code)

            issues: list[str] = []
            if not valid:
                issues.append(f"Syntax error: {err}")
            if tool_count_in_code < expected:
                issues.append(
                    f"Expected {expected} @tool functions, found {tool_count_in_code}"
                )

            if not issues:
                logger.info("  ✓ Valid Python with %d/%d tools (attempt %d)", tool_count_in_code, expected, attempt)
                break

            issue_str = "; ".join(issues)
            logger.warning("  Attempt %d validation failed: %s", attempt, issue_str)

            if attempt == max_attempts:
                logger.error("  All %d attempts exhausted. Best: %d/%d tools", max_attempts, tool_count_in_code, expected)

        # ── Step 2: Generate test file (with retry) ──────────────────────
        logger.info("Step 2: Generating test_server.py via LLM...")
        test_prompt = _build_test_prompt(spec, tools)
        raw_test = _call_llm(test_prompt, system_instruction=_SYSTEM_PROMPT, max_tokens=8192)
        test_code = _extract_code(raw_test)

        # Validate test has enough test functions
        test_func_count = len(re.findall(r"(?:async )?def test_", test_code))
        # We expect at minimum: test_list_tools + test_tool_schemas + 1 per tool
        min_tests = 2 + expected
        if test_func_count < min_tests:
            logger.warning("  Test file has %d tests, expected >= %d — regenerating...", test_func_count, min_tests)
            enhanced_test_prompt = _build_test_prompt(spec, tools) + (
                f"\n\nIMPORTANT: You MUST generate at least {min_tests} test functions:\n"
                f"- test_list_tools()\n- test_tool_schemas()\n"
                + "\n".join(f"- test_{t.name}()" for t in tools)
                + "\nGenerate ALL of them. Do NOT skip any."
            )
            raw_test2 = _call_llm(enhanced_test_prompt, system_instruction=_SYSTEM_PROMPT, max_tokens=16384)
            test_code2 = _extract_code(raw_test2)
            new_count = len(re.findall(r"(?:async )?def test_", test_code2))
            if new_count > test_func_count:
                test_code = test_code2
                logger.info("  Retry improved: %d → %d test functions", test_func_count, new_count)
            else:
                logger.info("  Retry did not improve (%d tests), keeping original", new_count)

        # ── Step 4: Write config files ───────────────────────────────────
        logger.info("Step 4: Writing config files...")

        requirements = "mcp[cli]>=1.0.0\nhttpx>=0.28\n"
        env_template = (
            f"# {spec.title} MCP Server Configuration\n"
            f"{env_prefix}_BASE_URL={spec.base_url}\n"
            f"{env_prefix}_API_KEY=your-api-key-here\n"
        )
        main_code = (
            '"""Entry point for Blaxel deployment."""\n\n'
            "from src.server import mcp\n\n"
            'if __name__ == "__main__":\n'
            '    mcp.run(transport="streamable-http")\n'
        )
        pyproject = (
            "[project]\n"
            f'name = "{name}"\n'
            'version = "0.1.0"\n'
            f'description = "Auto-generated MCP adapter for {spec.title}"\n'
            'requires-python = ">=3.11"\n'
            "dependencies = [\n"
            '    "mcp[cli]>=1.0.0",\n'
            '    "httpx>=0.27.0",\n'
            "]\n"
        )

        # Build blaxel.toml for Blaxel deployment
        env_section_lines = []
        env_section_lines.append(f'{env_prefix}_BASE_URL = "{spec.base_url}"')
        if spec.auth_schemes:
            env_section_lines.append(f'{env_prefix}_API_KEY = ""')

        blaxel_toml = (
            f'name = "{name}"\n'
            f'type = "function"\n'
            f'\n'
            f'[entrypoint]\n'
            f'prod = ".venv/bin/python3 src/server.py"\n'
            f'dev = "uv run python src/server.py"\n'
            f'\n'
            f'[env]\n'
            + "\n".join(env_section_lines) + "\n"
            f'\n'
            f'[[triggers]]\n'
            f'id = "trigger-{name}"\n'
            f'type = "http"\n'
            f'\n'
            f'[triggers.configuration]\n'
            f'path = "functions/{name}"\n'
            f'authenticationType = "public"\n'
        )

        output = GeneratedOutput(
            server_code=server_code, test_code=test_code,
            requirements=requirements, env_template=env_template,
            server_name=name, tool_count=len(tools),
        )

        if output_dir:
            out = Path(output_dir)
            out.mkdir(parents=True, exist_ok=True)
            # Blaxel expects server code in src/
            src_dir = out / "src"
            src_dir.mkdir(parents=True, exist_ok=True)
            (src_dir / "server.py").write_text(server_code, encoding="utf-8")
            (out / "test_server.py").write_text(test_code, encoding="utf-8")
            (out / "requirements.txt").write_text(requirements, encoding="utf-8")
            (out / ".env.example").write_text(env_template, encoding="utf-8")
            (out / "pyproject.toml").write_text(pyproject, encoding="utf-8")
            (out / "blaxel.toml").write_text(blaxel_toml, encoding="utf-8")
            output.output_dir = out
            logger.info("Wrote 6 files to %s", out)

    return output
